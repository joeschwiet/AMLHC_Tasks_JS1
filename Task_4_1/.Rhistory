lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
## Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
## Add row names as a new column in the data frame for readability
ranked_features$feature_name <- rownames(ranked_features)
## Print the ranked features with feature names and information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
## Print the ranked features with feature names and information gain
print(ranked_features)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Print the information gain for each attribute
print(info_gain)
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)[order(-info_gain$attr_importance), ]
print(ranked_features)
## Print the information gain values
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)
ranked_features <- ranked_features[order(-ranked_features$attr_importance), ]
## View the ranked features by information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)
ranked_features <- ranked_features[order(-ranked_features$attr_importance), ]
## View the ranked features by information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
## Print the information gain for each attribute
print(info_gain)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
# Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
# Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
# Add row names as a new column in the data frame for readability
ranked_features$feature_name <- rownames(ranked_features)
# Print the ranked features with feature names and information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
# Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
# Ensure that the row names, which are the feature names, are included as a column
info_gain_df$Feature <- rownames(info_gain_df)
# Remove the row names to avoid confusion
rownames(info_gain_df) <- NULL
# Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
# Print the sorted ranked features
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
# Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
# Ensure that the row names, which are the feature names, are included as a column
info_gain_df$Feature <- rownames(info_gain_df)
# Remove the row names to avoid confusion
rownames(info_gain_df) <- NULL
# Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
# Print the sorted ranked features
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
# Boxplot for the most discriminating feature
ggplot(diabetes_complete, aes(x = class, y = plas)) +
geom_boxplot() +
labs(title = "Boxplot of Plasma Glucose by Class",
x = "Class",
y = "Plasma Glucose Levels")
# Distribution plot for the most discriminating feature
ggplot(diabetes_complete, aes(x = plas, fill = class)) +
geom_histogram(binwidth = 10, position = "identity", alpha = 0.5) +
labs(title = "Distribution of Plasma Glucose by Class",
x = "Plasma Glucose Levels",
y = "Count") +
theme_minimal()
# Boxplot for the least discriminating feature
ggplot(diabetes_complete, aes(x = class, y = pedi)) +
geom_boxplot() +
labs(title = "Boxplot of Pedigree Function by Class",
x = "Class",
y = "Pedigree Function")
# Distribution plot for the least discriminating feature
ggplot(diabetes_complete, aes(x = pedi, fill = class)) +
geom_histogram(binwidth = 0.1, position = "identity", alpha = 0.5) +
labs(title = "Distribution of Pedigree Function by Class",
x = "Pedigree Function",
y = "Count") +
theme_minimal()
library(ggplot2)
# Extracting the scores for PC1 and PC2
scores_df <- data.frame(PC1 = pca_result$scores[,1], PC2 = pca_result$scores[,2])
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
# Load dataset food.csv
food_data <- read.csv("food.csv", header = TRUE, sep = ",")
# Analyze the dimensions of the dataset
dim_food_data <- dim(food_data) # Gets the dimensions of the data frame
cat("Dimensions of dataset: ", dim_food_data, "\n") # Prints out the dimensions
# Calculate the number of missing values in the dataset
num_missing_values <- sum(is.na(food_data)) # Counts the total number of NA values
cat("Number of missing values: ", num_missing_values, "\n") # Prints out the number of missing values
# Feature scaling using z-transformation (standardization)
scaled_food_data <- scale(food_data[,-1])
# To check the scaling, let's look at the summary of the first few features
# This gives the mean and standard deviation which should be 0 and 1, respectively, after scaling
summary_scaled_food_data <- summary(scaled_food_data)
print(summary_scaled_food_data) # Prints a summary statistics of the scaled data
# Perform PCA using the princomp function
pca_result <- princomp(scaled_food_data)
# Print a summary of the PCA results to get eigenvalues and the proportion of variance
summary(pca_result)
# View the loadings for the PCA components
loadings(pca_result)
# Eigenvalues can give us the variance explained by each principal component
pca_result$sdev^2
# To see the proportion of variance explained by the principal components
prop_var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
prop_var_explained
# Extracting the scores for PC1 and PC2
scores_df <- data.frame(PC1 = pca_result$scores[,1], PC2 = pca_result$scores[,2])
# Creating a score plot with PC1 and PC2
qplot(data = scores_df, x = PC1, y = PC2, xlab = "PC1", ylab = "PC2", main = "PCA Score Plot")
# Extracting the scores for PC1 and PC2
scores_df <- data.frame(PC1 = pca_result$scores[,1], PC2 = pca_result$scores[,2])
# Creating a score plot with PC1 and PC2
ggplot(data = scores_df, aes(x = PC1, y = PC2)) +
geom_point() +  # Adds a scatter plot layer
theme_minimal() +  # Optional: Applies a minimalistic theme
labs(title = "PCA Score Plot", x = "PC1 (Principal Component 1)", y = "PC2 (Principal Component 2)")  # Adds labels
```{r load dataset, echo = FALSE}
# Create the confusion matrix
install.packages(caret)
# Define the predicted and actual classes
predicted <- c("Benign", "Malignant", "Malignant")
actual <- c("Benign", "Benign", "Malignant")
# Create the confusion matrix
install.packages(caret)
library(caret)
# Create the confusion matrix
install.package(caret)
# Create the confusion matrix
install.packages(caret)
# Create the confusion matrix
install.packages("caret")
setwd(cd /Users/johannesschwietering/Library/Mobile\ Documents/com~apple~CloudDocs/Studium/UMIT/Modul\ 12/AMLHC_Tasks_JS1)
setwd("/Users/johannesschwietering/Library/Mobile Documents/com~apple~CloudDocs/Studium/UMIT/Modul 12/AMLHC_Tasks_JS1")
setwd("/Users/johannesschwietering/Library/Mobile Documents/com~apple~CloudDocs/Studium/UMIT/Modul 12/AMLHC_Tasks_JS1/Task_4_1")
> setwd("/Users/johannesschwietering/Library/Mobile Documents/com~apple~CloudDocs/Studium/UMIT/Modul 12/AMLHC_Tasks_JS1/Task_4_2")
setwd("/Users/johannesschwietering/Library/Mobile Documents/com~apple~CloudDocs/Studium/UMIT/Modul 12/AMLHC_Tasks_JS1/Task_4_2")
knitr::opts_chunk$set(echo = TRUE)
# Load the diabetes dataset
diabetes_data <- read.csv("diabetes.csv")
# View the first few rows of the dataset
head(diabetes_data)
library(caret)
# Create a partition with createDataPartition
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(diabetes_data$Outcome, p = 0.75, list = FALSE, times = 1)
trainIndex <- createDataPartition(diabetes_data$class, p = 0.75, list = FALSE, times = 1)
# Create the training dataset
trainSet <- diabetes_data[trainIndex, ]
# Create the test dataset
testSet <- diabetes_data[-trainIndex, ]
# Check the dimensions of the training and test sets
dim(trainSet)
dim(testSet)
library(randomForest)
# Define the tuning grid using expand.grid
tuningGrid <- expand.grid(mtry = 2:8)
# View the tuning grid
print(tuningGrid)
setwd("cd /Users/johannesschwietering/Library/Mobile\ Documents/com~apple~CloudDocs/Studium/UMIT/Modul\ 12/AMLHC_Tasks_JS1/Task_4_1")
setwd("/Users/johannesschwietering/Library/Mobile\ Documents/com~apple~CloudDocs/Studium/UMIT/Modul\ 12/AMLHC_Tasks_JS1/Task_4_1")
knitr::opts_chunk$set(echo = TRUE)
# Load the diabetes dataset
heart_data <- read.csv("heartdata.csv")
# View the first few rows of the dataset
head(heart_data)
str(heart_data)
knitr::opts_chunk$set(echo = TRUE)
# Load the diabetes dataset
heart_data <- read.csv("heartdata.csv")
# View the first few rows of the dataset
head(heart_data)
str(heart_data)
# Assumption 1: Linearity
# Checking linearity between 'biking' and 'heartdisease' and 'smoking' and 'heartdisease'
plot(heart_data$biking, heart_data$heartdisease, main = "Scatter plot of biking vs heart disease",
xlab = "Biking", ylab = "Heart Disease")
plot(heart_data$smoking, heart_data$heartdisease, main = "Scatter plot of smoking vs heart disease",
xlab = "Smoking", ylab = "Heart Disease")
# Assumption 2: Normality of residuals
# Fitting a simple linear model to check residuals
model <- lm(heartdisease ~ biking + smoking, data = heart_data)
residuals <- resid(model)
# Plotting histogram of residuals
hist(residuals, main = "Histogram of Residuals", xlab = "Residuals", breaks = 30,
col = "blue")
# Assumption 3: Homoscedasticity
# Plotting residuals vs. fitted values to check for constant variance
plot(fitted(model), residuals, main = "Residuals vs Fitted Values",
xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
# Assumption 4: Absence of multicollinearity
# Checking correlations among explanatory variables
cor.test(heart_data$biking, heart_data$smoking, method = "pearson")
# Assuming 'heart_data' is already loaded in your R environment
# Assumption 1: Independence of observations
# Note: Typically checked by understanding the data collection process or using time-series analysis techniques
# Not applicable here directly because we assume one observation per subject with no time component
# Assumption 2: Normality of the dependent variable
hist(heart_data$dependent_variable, main = "Histogram of Dependent Variable",
xlab = "Dependent Variable", breaks = 30, col = "darkgray")
# Assumption 2: Normality of the dependent variable
hist(heart_data$heartdisease, main = "Histogram of Heartdisease",
xlab = "Dependent Variable", breaks = 30, col = "darkgray")
# Assumption 3: Linearity
plot(heart_data$biking, heart_data$heartdisease,
main = "Scatter Plot of biking vs. heart_disease",
xlab = "Independent Variable", ylab = "Dependent Variable",
pch = 19, col = "blue")  # pch = 19 makes the points solid, col = "blue" colors them blue
plot(heart_data$smoking, heart_data$heartdisease,
main = "Scatter Plot of smoking vs.  heart_disease",
xlab = "Independent Variable", ylab = "Dependent Variable",
pch = 19, col = "blue")  # pch = 19 makes the points solid, col = "blue" colors them blue
# Fit a linear model to check for linearity visually and for Homoscedasticity
model <- lm(dependent_variable ~ independent_variable, data = heart_data)
# Fit a linear model to check for linearity visually and for Homoscedasticity
model <- lm(heartdisease ~ biking, data = heart_data)
model <- lm(heartdisease ~ smoking, data = heart_data)
# Fit a linear model to check for linearity visually and for Homoscedasticity
model1 <- lm(heartdisease ~ biking, data = heart_data)
model2 <- lm(heartdisease ~ smoking, data = heart_data)
# Check linearity and homoscedasticity with a plot of residuals vs. fitted values
plot(fitted(model), resid(model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values", ylab = "Residuals",
pch = 19, col = "red")  # pch = 19 makes the points solid, col = "red" colors them red
abline(h = 0, lty = 2, col = "gray")  # Adds a horizontal line at 0 for reference
# Check linearity and homoscedasticity with a plot of residuals vs. fitted values
plot(fitted(model), resid(model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values", ylab = "Residuals",
pch = 19, col = "red")  # pch = 19 makes the points solid, col = "red" colors them red
plot(fitted(model), resid(model2),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values", ylab = "Residuals",
pch = 19, col = "red")  # pch = 19 makes the points solid, col = "red" colors them red
# Enhanced Homoscedasticity test with scale-location plot
# This plot shows if residuals are spreading equally along the ranges of predictors
plot(fitted(model1), sqrt(abs(resid(model))),
main = "Scale-Location Plot",
xlab = "Fitted Values", ylab = "Square Root of |Residuals|",
pch = 19, col = "green")  # pch = 19 makes the points solid, col = "green" colors them green
# Check linearity and homoscedasticity with a plot of residuals vs. fitted values
plot(fitted(model), resid(model1),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values", ylab = "Residuals",
pch = 19, col = "red")  # pch = 19 makes the points solid, col = "red" colors them red
plot(fitted(model), resid(model2),
main = "Residuals vs. Fitted Values",
xlab = "Fitted Values", ylab = "Residuals",
pch = 19, col = "red")  # pch = 19 makes the points solid, col = "red" colors them red
# Assumption 2: Normality of the dependent variable
hist(heart_data$heartdisease, main = "Histogram of Heartdisease",
xlab = "Dependent Variable", breaks = 30, col = "darkgray")
# Assumption 3: Linearity
plot(heart_data$biking, heart_data$heartdisease,
main = "Scatter Plot of biking vs. heart_disease",
xlab = "Independent Variable", ylab = "Dependent Variable",
pch = 19, col = "blue")  # pch = 19 makes the points solid, col = "blue" colors them blue
plot(heart_data$smoking, heart_data$heartdisease,
main = "Scatter Plot of smoking vs.  heart_disease",
xlab = "Independent Variable", ylab = "Dependent Variable",
pch = 19, col = "blue")  # pch = 19 makes the points solid, col = "blue" colors them blue
# Fit a linear model
model <- lm(heartdisease ~ biking, data = heart_data)
# Fit a linear model
model1 <- lm(heartdisease ~ biking, data = heart_data)
model1
model2 <- lm(heartdisease ~ smoking, data = heart_data)
model2
summary(model1)
summary(model2)
model3 <- lm(heartdisease ~ biking + smoking, data=heart_data)
model3 <- lm(heartdisease ~ biking + smoking, data=heart_data)
par(mfrow_c(2,2))
par(mfrow=c(2,2))
plot(model3)
par(mfrow=c(2,2))
par(mfrow=c(2,2))
plot(model3)
library(caret)
require(caret)
fitControl <- trainControl(##10-fold CV
method = "cv",
number = 10)
lmFit <- train(heartdisease ~ ., data=heart_data, method = "lm", preProcess=preProcess("center", "scale"), trControl=fitControl)
lmFit <- train(heartdisease ~ ., data=heart_data, method = "lm", preProcess=preProcess(method=c("center", "scale"), trControl=fitControl)
lmFit <- train(heartdisease ~ ., data=heart_data, method = "lm", trControl=fitControl)
lmFit <- train(heartdisease ~ ., data=heart_data, method="lm", trControl=fitControl)
lmFit
lmFit <- train(heartdisease ~ ., data=heart_data,preProcess=preProcess(method=c("center", "scale"), method="lm", trControl=fitControl)
lmFit
lmFit <- train(heartdisease ~ ., data=heart_data,preProcess=preProcess(method=c("center", "scale"), method="lm", trControl=fitControl)
print(lmFit)
summary(lmFit)
lmFit <- train(heartdisease ~ ., data=heart_data, preProcess=c("center", "scale"), method="lm", trControl=fitControl)
summary(lmFit)
lmFit
library(foreign)
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
# Load the diabetes dataset
diabetes_data <- read.arff("diabetes.arff")
# View the first few rows of the dataset
head(diabetes_data)
write.csv(diabetes_data, "diabetes.csv")
diaFit <- glm(class ~ ., data=diabetes_data)
diaFit <- glm(class ~ ., data=diabetes_data, family="binomial")
summary(diaFit)
glmFit <- train(class ~ ., data=class, method="glm", trControl=fitControl, preProc=c("center","scale"))
glmFit <- train(class ~ ., data=diabetes_data, method="glm", trControl=fitControl, preProc=c("center","scale"))
glmFit <- train(class ~ ., data=diabetes_data, method="glm", trControl=fitControl, preProc=c("center","scale"))
summary(glmFit)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
# Assuming 'heart_data' is already loaded in your R environment
# Assumption 1: Independence of observations
# Note: Typically checked by understanding the data collection process or using time-series analysis techniques
# Not applicable here directly because we assume one observation per subject with no time component
# Assumption 2: Normality of the dependent variable
hist(heart_data$heartdisease, main = "Histogram of Heartdisease",
xlab = "Dependent Variable", breaks = 30, col = "darkgray")
# Assumption 3: Linearity
plot(heart_data$biking, heart_data$heartdisease,
main = "Scatter Plot of biking vs. heart_disease",
xlab = "Independent Variable", ylab = "Dependent Variable",
pch = 19, col = "blue")  # pch = 19 makes the points solid, col = "blue" colors them blue
plot(heart_data$smoking, heart_data$heartdisease,
main = "Scatter Plot of smoking vs.  heart_disease",
xlab = "Independent Variable", ylab = "Dependent Variable",
pch = 19, col = "blue")  # pch = 19 makes the points solid, col = "blue" colors them blue
summary(model1)
summary(model2)
summary(model3)
par(mfrow=c(2,2))
plot(model3)
par(mfrow=c(2,2))
plot(model3)
lmFit <- train(heartdisease ~ ., data=heart_data, preProcess=c("center", "scale"), method="lm", trControl=fitControl)
lmFit
summary(lmFit)
print(lmFit)
summary(diaFit)
summary(glmFit)
summary(diaFit)
library(caret)
glmFit <- train(class ~ ., data=diabetes_data, method="glm", trControl=fitControl, preProc=c("center","scale"))
summary(glmFit)
fitControl <- trainControl(method = "cv", number = 10, savePredictions = "final")
glmFit <- train(class ~ ., data=diabetes_data, method="glm", trControl=fitControl, preProc=c("center","scale"))
summary(glmFit)
