# Rank features using information gain (package FSelector)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
print(description)
install.packages("FSelector")
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
library(FSelector)
library(rJava)
library(FSelector)
str(diabetes_complete)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Print the information gain for each attribute
print(info_gain)
## Print the information gain for each attribute
print(info_gain)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
#install.packages("Hmisc")
#install.packages("htmltools")
#install.packages("FSelector")
#install.packages("rJava")
library(Hmisc)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Print the information gain for each attribute
print(info_gain)
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)[order(-info_gain$attr_importance), ]
print(ranked_features)
## Print the information gain values
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)
ranked_features <- ranked_features[order(-ranked_features$attr_importance), ]
## View the ranked features by information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
## Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
## Add row names as a new column in the data frame for readability
ranked_features$feature_name <- rownames(ranked_features)
## Print the ranked features with feature names and information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
## Print the ranked features with feature names and information gain
print(ranked_features)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Print the information gain for each attribute
print(info_gain)
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)[order(-info_gain$attr_importance), ]
print(ranked_features)
## Print the information gain values
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)
ranked_features <- ranked_features[order(-ranked_features$attr_importance), ]
## View the ranked features by information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
## Rank the features by information gain
ranked_features <- as.data.frame(info_gain)
ranked_features <- ranked_features[order(-ranked_features$attr_importance), ]
## View the ranked features by information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
## Print the information gain for each attribute
print(info_gain)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
# Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
# Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
# Add row names as a new column in the data frame for readability
ranked_features$feature_name <- rownames(ranked_features)
# Print the ranked features with feature names and information gain
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
# Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
# Ensure that the row names, which are the feature names, are included as a column
info_gain_df$Feature <- rownames(info_gain_df)
# Remove the row names to avoid confusion
rownames(info_gain_df) <- NULL
# Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
# Print the sorted ranked features
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Replace zeros with NAs for all numeric columns
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], function(x) replace(x, x == 0, NA))
# Define and apply a function for IQR based outlier detection and replace the detected outliers function to replace outliers with NA
replace_outliers_with_NA <- function(x) {
Q1 <- quantile(x, 0.25, na.rm = TRUE)
Q3 <- quantile(x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- NA
return(x)
}
# Apply the function to replace outliers with NA in each numeric column
diabetes_data[sapply(diabetes_data, is.numeric)] <- lapply(diabetes_data[sapply(diabetes_data, is.numeric)], replace_outliers_with_NA)
# Now, select only the complete cases
diabetes_complete <- diabetes_data[complete.cases(diabetes_data), ]
str(diabetes_complete)
# [Optional: Describe single features (R function describe of package Hmisc)]
description <- describe(diabetes_complete)
print(description)
# Rank features using information gain (package FSelector)
## Convert the class variable to a factor
diabetes_complete$class <- as.factor(diabetes_complete$class)
## Calculate information gain for each feature
info_gain <- information.gain(class ~ ., diabetes_complete)
# Convert the results into a readable data frame format
info_gain_df <- as.data.frame(info_gain)
# Ensure that the row names, which are the feature names, are included as a column
info_gain_df$Feature <- rownames(info_gain_df)
# Remove the row names to avoid confusion
rownames(info_gain_df) <- NULL
# Sort the features by their information gain values in descending order
ranked_features <- info_gain_df[order(-info_gain_df$attr_importance), ]
# Print the sorted ranked features
print(ranked_features)
# Create boxplot and distribution plots of one discriminating (i.e., highest IG score) and one non-discriminating feature (i.e., smallest IG score)
# Boxplot for the most discriminating feature
ggplot(diabetes_complete, aes(x = class, y = plas)) +
geom_boxplot() +
labs(title = "Boxplot of Plasma Glucose by Class",
x = "Class",
y = "Plasma Glucose Levels")
# Distribution plot for the most discriminating feature
ggplot(diabetes_complete, aes(x = plas, fill = class)) +
geom_histogram(binwidth = 10, position = "identity", alpha = 0.5) +
labs(title = "Distribution of Plasma Glucose by Class",
x = "Plasma Glucose Levels",
y = "Count") +
theme_minimal()
# Boxplot for the least discriminating feature
ggplot(diabetes_complete, aes(x = class, y = pedi)) +
geom_boxplot() +
labs(title = "Boxplot of Pedigree Function by Class",
x = "Class",
y = "Pedigree Function")
# Distribution plot for the least discriminating feature
ggplot(diabetes_complete, aes(x = pedi, fill = class)) +
geom_histogram(binwidth = 0.1, position = "identity", alpha = 0.5) +
labs(title = "Distribution of Pedigree Function by Class",
x = "Pedigree Function",
y = "Count") +
theme_minimal()
library(ggplot2)
# Extracting the scores for PC1 and PC2
scores_df <- data.frame(PC1 = pca_result$scores[,1], PC2 = pca_result$scores[,2])
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
# Load dataset food.csv
food_data <- read.csv("food.csv", header = TRUE, sep = ",")
# Analyze the dimensions of the dataset
dim_food_data <- dim(food_data) # Gets the dimensions of the data frame
cat("Dimensions of dataset: ", dim_food_data, "\n") # Prints out the dimensions
# Calculate the number of missing values in the dataset
num_missing_values <- sum(is.na(food_data)) # Counts the total number of NA values
cat("Number of missing values: ", num_missing_values, "\n") # Prints out the number of missing values
# Feature scaling using z-transformation (standardization)
scaled_food_data <- scale(food_data[,-1])
# To check the scaling, let's look at the summary of the first few features
# This gives the mean and standard deviation which should be 0 and 1, respectively, after scaling
summary_scaled_food_data <- summary(scaled_food_data)
print(summary_scaled_food_data) # Prints a summary statistics of the scaled data
# Perform PCA using the princomp function
pca_result <- princomp(scaled_food_data)
# Print a summary of the PCA results to get eigenvalues and the proportion of variance
summary(pca_result)
# View the loadings for the PCA components
loadings(pca_result)
# Eigenvalues can give us the variance explained by each principal component
pca_result$sdev^2
# To see the proportion of variance explained by the principal components
prop_var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
prop_var_explained
# Extracting the scores for PC1 and PC2
scores_df <- data.frame(PC1 = pca_result$scores[,1], PC2 = pca_result$scores[,2])
# Creating a score plot with PC1 and PC2
qplot(data = scores_df, x = PC1, y = PC2, xlab = "PC1", ylab = "PC2", main = "PCA Score Plot")
# Extracting the scores for PC1 and PC2
scores_df <- data.frame(PC1 = pca_result$scores[,1], PC2 = pca_result$scores[,2])
# Creating a score plot with PC1 and PC2
ggplot(data = scores_df, aes(x = PC1, y = PC2)) +
geom_point() +  # Adds a scatter plot layer
theme_minimal() +  # Optional: Applies a minimalistic theme
labs(title = "PCA Score Plot", x = "PC1 (Principal Component 1)", y = "PC2 (Principal Component 2)")  # Adds labels
```{r load dataset, echo = FALSE}
# Create the confusion matrix
install.packages(caret)
# Define the predicted and actual classes
predicted <- c("Benign", "Malignant", "Malignant")
actual <- c("Benign", "Benign", "Malignant")
# Create the confusion matrix
install.packages(caret)
library(caret)
# Create the confusion matrix
install.package(caret)
# Create the confusion matrix
install.packages(caret)
# Create the confusion matrix
install.packages("caret")
setwd("cd /Users/johannesschwietering/Library/Mobile\ Documents/com~apple~CloudDocs/Studium/UMIT/Modul\ 12/AMLHC_Tasks_JS1/Task_4_3")
setwd("Users/johannesschwietering/Library/Mobile\ Documents/com~apple~CloudDocs/Studium/UMIT/Modul\ 12/AMLHC_Tasks_JS1/Task_4_1")
setwd("/Users/johannesschwietering/Library/Mobile Documents/com~apple~CloudDocs/Studium/UMIT/Modul 12/AMLHC_Tasks_JS1/Task_4_3")
setwd("/Users/johannesschwietering/Library/Mobile Documents/com~apple~CloudDocs/Studium/UMIT/Modul 12/AMLHC_Tasks_JS1/Task_4_3")
require(caret)
data(BloodBrain)
data(BloodBrain)
View(bbbDescr)
X <- bbbDescr
y <- logBBB
#lets check if that worked
length(y)
#lets check if that worked
summary(bbDescr)
#lets check if that worked
str(X)
inTrain <- createDataPartition(y, 0.75, list=FALSE)
inTrain <- createDataPartition(y, p=0.75, list=FALSE)
# lets checj if that worked
length(inTrain)
#
X_train <- X[inTrain,]
y_train <- y[inTrain]
X_test <- X[-inTrain,]
y_test <- y[-inTrain]
# lets check if that worked
dim(X-train)
# lets check if that worked
dim(X_train)
dim(y_train)
trControl <- trainControl(method="cv", number = 10)
model_rf <- train(X_train, y_train, method="rf", preProcess = c())
trControl <- trainControl(method="cv", number = 10)
model_rf <- train(X_train, y_train, method="rf", preProcess = c("center","scale"), trainControl=trControl)
featVar <- apply(X_train, 2, var)
X_train <- X_train[,featVar>0]
X_train <- X_train[,featVar>0]
X_trest <- X_test[,featVar>0]
dim(X_train)
#lets check if that worked
str(X)
#
X_train <- X[inTrain,]
y_train <- y[inTrain]
X_test <- X[-inTrain,]
y_test <- y[-inTrain]
require(caret)
data(BloodBrain)
X <- bbbDescr
y <- logBBB
#lets check if that worked
str(X)
length(y)
#create Data partition
inTrain <- createDataPartition(y, p=0.75, list=FALSE)
# lets checj if that worked, length should be around 150
length(inTrain)
#
X_train <- X[inTrain,]
y_train <- y[inTrain]
X_test <- X[-inTrain,]
y_test <- y[-inTrain]
# lets check if that worked
dim(X_train)
dim(y_train)
dim(X_train)
vi <- varImp(model_rf)
best_rf <- model_rf$finalModel
y_predicted <- predict(best_rf, X_test)
y_predicted
RMSE(y_predicted, y_test)
RMSE(y_predicted, y_test)
RMSE(y_predicted, y_test)
#lets check if that worked
str(X)
length(y)
setwd("/Users/johannesschwietering/Library/Mobile Documents/com~apple~CloudDocs/Studium/UMIT/Modul 12/AMLHC_Tasks_JS1/Task_4_4")
data(cox2)
X <- cox2Descr
y <- cox2Class
#lets check if that worked
str(X)
length(y)
str(y)
#create Data partition
inTrain <- createDataPartition(y, p=0.75, list=FALSE)
#create Data partition
inTrain <- createDataPartition(y, p=0.75, list=FALSE)
# lets checj if that worked, length should be around 150
length(inTrain)
#
X_train <- X[inTrain,]
y_train <- y[inTrain]
X_test <- X[-inTrain,]
y_test <- y[-inTrain]
# lets check if that worked
dim(X_train)
dim(y_train)
trControl <- trainControl(method="cv", number = 10)
model_rf <- train(X_train, y_train, method="rf", preProcess = c("center","scale"), trainControl=trControl)
#lets see if it worked
model_rf
plot(varImp(model_rf))
plot(varImp(model_rf),[1:20])
plot(varImp(model_rf)[1:20])
vidf <- varImp(model_rf)$importance
view(vidf)
str(vidf)
best_rf <- model_rf$finalModel
y_predicted <- predict(best_rf, X_test)
confusionMatrix()
y_predicted <- predict(best_rf, X_test)
confusionMatrix(y_predicted, y_test)
confusionMatrix(y_predicted, y_train)
confusionMatrix(y_predicted, y_test)
confusionMatrix(y_predicted, y_test)
#lets see if it worked
model_rf
best_rf <- model_rf$finalModel
y_predicted <- predict(best_rf, X_test)
confusionMatrix(y_predicted, y_test)
# lets see if it worked
y_predicted
RMSE(y_predicted, y_test)
# lets see if it worked
y_predicted
