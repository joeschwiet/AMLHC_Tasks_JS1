---
title: "Task 2.1 Unsupervised preprocessing and visualization"
author: "Johannes Schwietering"
date: "2024-04-05"
output: html_document
---
Load dataset food.csv


```{r setup}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)

# Load dataset food.csv
food_data <- read.csv("food.csv", header = TRUE, sep = ",")

```

## Get an overview by analyzing dimension, number of missing values and feature scaling


```{r overview}
# Analyze the dimensions of the dataset
dim_food_data <- dim(food_data) # Gets the dimensions of the data frame
cat("Dimensions of dataset: ", dim_food_data, "\n") # Prints out the dimensions

# Calculate the number of missing values in the dataset
num_missing_values <- sum(is.na(food_data)) # Counts the total number of NA values
cat("Number of missing values: ", num_missing_values, "\n") # Prints out the number of missing values

```

The results show that we have a dataframe with 25 rows and 10 columns. We ha no missing values. Die Z-Transformation hat funktioniert: Der Mittelwert ist für alle Featuers 0. Jetzt können wir die einzelnen Features sinnvoll miteinander vergleichen. 


## Preprocess data using z-transformation (R function scale)

```{r pressure}
# Feature scaling using z-transformation (standardization)
scaled_food_data <- scale(food_data[,-1])

# To check the scaling, let's look at the summary of the first few features
# This gives the mean and standard deviation which should be 0 and 1, respectively, after scaling
summary_scaled_food_data <- summary(scaled_food_data) 
print(summary_scaled_food_data) # Prints a summary statistics of the scaled data
```

The code standardizes the food data by applying a z-transformation, ensuring each feature has a mean of 0 and a standard deviation of 1, which is confirmed by the summary statistics. The summary output for each variable shows that the mean is centered around 0, and the spread of the data around the mean is standardized, which is a common preprocessing step for many statistical analyses and machine learning models.

## Perform a principal component analysis (R function princomp)

```{r principal component analysis}
# Perform PCA using the princomp function
pca_result <- princomp(scaled_food_data)

# Print a summary of the PCA results to get eigenvalues and the proportion of variance
summary(pca_result)

# View the loadings for the PCA components
loadings(pca_result)

# Eigenvalues can give us the variance explained by each principal component
pca_result$sdev^2

# To see the proportion of variance explained by the principal components
prop_var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
prop_var_explained
```
The PCA on the scaled_food_data reveals that the first principal component (PC1) explains 44.52% of the variance within the data, and the first two components combined explain 62.68% of the variance. The loadings show the correlation of each original variable with the components, indicating which variables contribute most to each principal component. For instance, 'RedMeat', 'WhiteMeat', and 'Eggs' have strong loadings on PC1, implying they are important variables for this component. This PCA result allows for the reduction of the dataset's dimensionality while still retaining a significant portion of the original variance.

## Create a score plot using PC1 and PC2 (R function qplot)
```{r score plot}
# Extracting the scores for PC1 and PC2
scores_df <- data.frame(PC1 = pca_result$scores[,1], PC2 = pca_result$scores[,2])

# Creating a score plot with PC1 and PC2
ggplot(data = scores_df, aes(x = PC1, y = PC2)) +
  geom_point() +  # Adds a scatter plot layer
  theme_minimal() +  # Optional: Applies a minimalistic theme
  labs(title = "PCA Score Plot", x = "PC1 (Principal Component 1)", y = "PC2 (Principal Component 2)")  # Adds labels

````

The PCA score plot illustrates that PC1 captures a substantial amount of variance within the dataset, while PC2 captures additional but comparatively less variance.